---
title: "(Very) basic steps on how to weight a survey sample"
author: "JosepER"
subtitle: An explained example with R code
output:
  html_document:
    theme: united
    toc: yes
  html_notebook:
    theme: united
    toc: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
---

```{r, echo=TRUE, warning=FALSE, message=FALSE}

library(MASS)
library(glmnet)
library(caret)
library(survey)
library(readxl)
library(stringr)
library(forcats)
library(foreign)
library(magrittr)
library(tidyverse)
options(scipen = 9999)

set.seed(456162)

```


## Introduction

### Aim of this guide and further readings

This is a short introductory guide that shows the basic procedures to weight a survey. Please keep in mind that
there are many different ways of doing this. This guide intends to be a practical document and a step-by-step walkthrough for weighting a survey. It provides [R code] (https://www.r-project.org/) for all actions: from reading, manipulating and presenting data to modelling and callibration. This should allow readers to reproduce procerudes and results as well as to inspect objects at any given part of this guide. The source code in an 'R notebook' format can be found and publicly accessed from the [author's github page](https://github.com/JosepER/PPMI_how_to_weight_a_survey/01-_Basic_steps_on_how_to_weight_a_survey_sample.Rmd). For questions, clarifications or suggestions feel free to contact him at *jespasareig at gmail.com*. It is relevant to say that this text intentionally avoids explaining complex or advanced methods. Instead, it aims at providing users with a standard way of weighting and a limited number of variations.

Next sections will first give a very broad glimpse at all [main survey weighting steps](#basic steps in weighting a survey).The second section deals with importing data into R, data manipulation and briefly [presenting the dataset used for this guide](#exploring and presenting the dataset). Readers interested in a specific step, familiar with the 7th round of the European Social Survey or that want to jump directly into weighting procedures can skip this part of the guide. The three sections are the main components of this guide and show how to compute [design weights](#step 1: Design weights), [non-response weights](#step 2: Non-response weights) and [calibration weights](#step 3: Use of auxiliary data for weight calibration). Two more sections will be added to this survey in the future. These correspond to the analysis of weight variability and computing weighted estimates.

For more information you can check the following introductory texts:

  * Valliant et al (2013) *Practical Tools for Designing and Weighting Survey Samples*. New York: Springer Science+Business Media.
  * Lohr (2009) *Sampling: Design and Analysis*. 2nd Edition. Boston: Books/Cole.
  
And the book accompaining the R 'survey' package:  

  * Lumley (2010) *Complex Surveys: A Guide to Analysis Using R*. New Jersey: John Wiley & Sons Inc.

It might be also worth keeping an eye on the (still incipient) R package [*srvyr*](https://cran.r-project.org/web/packages/srvyr/index.html), developed and mantained by Greg Freedman Ellis.

Note: This guide focuses on surveys based on 'probability sample'.
These are surveys where all units in our statistical population have a chance of being selected
and the probability of selection is known to the researcher. A brief note on how to weight non-probability samples is included at the end of the guide. 

### Basic steps in weighting a survey

Weights are applied to reduce survey bias. In plain words, weighting consists on making our sample of survey respondents (more) representative of our statistical population. By statistical population we mean all those units for which we want to compute estimates.

There are four basic steps in weighting. These are:

1. __Base/design weights__
2. __Non-response weights__
3. __Use of auxiliary data/calibration__
4. __Analysis of weight variability/trimming__

The first step consists on computing weights to take into account the differences of units in the probability of being sampled.
'Being sampled' means being selected from the survey frame (i.e. the list of all units) to be approached for a survey response.
This step can be skipped if all units in the survey frame have the same probability of being sampled. This happens, for example:
* when all units in the survey frame are approached for the sample or; 
* with certain sampling designs (such as 'simple random sampling without replacement' or 'stratified random sampling without replacement' with distribution of sampled units across stratums proportional to the number of units in each stratum). These are usually called 'self-weighted' surveys. 

In the second step we need to adjust our responses by the differences in the probability of sampled units to reply to our survey. Our estimates would be biased if some profile of sampled units had higher propensity to reply than another and these profiles had differences in the dependent variables (i.e. our variables of interest). In this step, we need to estimate the probability of response using information available for both respondents and non-respondents. Non-response adjustment is not needed if all sampled units responded to the survey (i.e. in probability sampling surveys with 100% response rates).

The third step consists on adjusting our weights using available information about total population estimates. Note that this requieres data that is different from that needed in non-response adjustment (second step). Here we need auxiliary data which tells us information (i.e. estimates such as proportions, means, sums, counts) about the statistical population. The same variables should be available from our respondents but here we don't need information about non-respondents.

The last step is to check the variablity in our computed weights. High variation in weights can lead to some observations having too much importance in our sample. Even if weights reduce bias, they might largely inflate variance of estimates. Therefore, some survey practitioners worry about dealing with highly unequal weights.


## Read data and data management

We first need to import data into R. In this guide we will use data from the 7th round of the [European Social Survey](http://www.europeansocialsurvey.org/). The advantage of this data is that the European Social Survey (ESS) is a well documented and high quality probability survey. It allows us to understand how responses were collected and provides some useful information about non-respondents. At the same time, the 7th ESS was weighted by expert statisticians. The process of the two phases of weighting they applied is explained in their [website](http://www.europeansocialsurvey.org/methodology/ess_methodology/data_processing_archiving/weighting.html). This will allow us to compare our own weights and results with those already computed by their team of experts.

For this guide we will use the following 7th ESS datafiles in SPSS ('.sav') format: 

* [sample data (SDDF), edition 1.1](http://www.europeansocialsurvey.org/download.html?file=ESS7SDDFe01_1&y=2014), which contains the  probability of being sampled for all respondents and non-respondents invited to the survey;  
* [the data from Contact forms, edition 2.1](http://www.europeansocialsurvey.org/download.html?file=ESS7CFe02_1&y=2014), which provides information about the process of data collection (e.g. number of times the person was approached for a response, ID of interviewer in each approach, conditions of the house/area where the potential respondent lived.). We will call this data the **'paradata'** of the survey.


#### Import data

```{r echo=TRUE, warning=FALSE, message=FALSE}

sample.data <- read.spss("data/ESS7SDDFe1_1.sav", to.data.frame = T)  %>%
  filter(cntry == "United Kingdom")

paradata <- read.spss("data/ess7CFe02_1.sav", to.data.frame = T) %>%
  filter(cntry == "United Kingdom") 

responses <- read.spss("data/ESS7e02_1.sav", to.data.frame = T) %>%
  filter(cntry == "United Kingdom") 

original.weights <- responses %>% select(idno ,dweight, pspwght, pweight)

```

#### Select variables

We select the variables we are going to use in our analysis. Here we just
write the names of the variables we intend to use.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

vars.sample.data <- c("idno", "psu", "prob")

# Note: In sample file, domain and stratify variables could be useful for other countries.

vars.paradata <- c("idno", "typesamp", "interva", "telnum", 
                   "agea_1", "gendera1", "type", "access", 
                   "physa", "littera", "vandaa")

# Note: In paradata file, age of sampled unit could be useful for other countries.    

resp.id <- c("idno")

resp.y <- c("cgtsmke", "cgtsday",
         "alcfreq", "alcwkdy", "alcwknd")

resp.x <- c("vote", "prtvtbgb",
            "prtclbgb", "prtdgcl",
            "ctzcntr", "ctzshipc",
         "brncntr","cntbrthc",
         "gndr", "agea", "hhmmb","eisced", "region",
         "pdwrk", "edctn", "uempla", "uempli", "rtrd",
         "wrkctra", "hinctnta")

```

Then we keep the variable labels (although these are not common in R).

```{r, echo=TRUE, warning=FALSE, message=FALSE}

selected.labels.sample.data <- attributes(sample.data)$variable.labels[which(names(sample.data) %in% vars.sample.data)]

selected.labels.paradata <- attributes(paradata)$variable.labels[which(names(paradata) %in% vars.paradata)]

selected.labels.responses <- attributes(responses)$variable.labels[which(names(responses) %in% c(resp.y, resp.x))] 

attributes(responses)$variable.labels %>% 
  cbind(names(responses),.) %>% 
  as_data_frame %>% 
  write_csv("interim_output/variable_labels.csv")

```

Now we do the selection of variables from the three data sets. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}

sample.data %<>% 
  .[vars.sample.data]

paradata %<>%
  .[vars.paradata]

responses %<>%
  .[which(names(responses) %in% c(resp.id, resp.y, resp.x))]

```


#### Merging datafiles

We merge the 'paradata' file containing all sampled units with the 'survey responses' file.
In a real situation, there would also be a 'survey frame' dataset. This would be a third data file
from which the sample was selected. Ideally, the 'survey frame' would include all units from the 
population.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

data <- paradata %>%
  left_join(sample.data, by = "idno") %>%
  left_join(responses, by = "idno") %>%
  arrange(interva)

rm(paradata,
   sample.data,
   responses)

```

And we add the labels we kept before.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

# Note: There are other variables that could be selected from paradata (i.e. result of visits, refusals, etc.)

attributes(data)$variable.labels <- c(selected.labels.paradata, selected.labels.sample.data[!names(selected.labels.sample.data) %in% "idno"],
                                      selected.labels.responses)
```


#### Recoding

Recoding daily cigarette consumption. All those that responded that don't smoke should have a 0.
```{r, echo=TRUE, warning=FALSE, message=FALSE}
data$cgtsday[data$cgtsmke %in% c("I have never smoked",
                                 "I don't smoke now but I used to",
                                 "I have only smoked a few times")] <- 0

data$alcohol_day <- NA 
data$alcohol_day <- (data$alcwkdy * 5 + data$alcwknd *2)/7 

data$alcohol_day[which(data$alcfreq == "Several times a week")] <- data$alcohol_day / 2.5
data$alcohol_day[which(data$alcfreq == "Once a week")] <- data$alcohol_day/7
data$alcohol_day[which(data$alcfreq == "2-3 times a month")] <- data$alcohol_day/10
data$alcohol_day[which(data$alcfreq == "Once a month")] <- data$alcohol_day/30
data$alcohol_day[which(data$alcfreq == "Less than once a month")] <- data$alcohol_day/50
data$alcohol_day[which(data$alcfreq == "Never")] <- 0

resp.y <- c(resp.y, "alcohol_day")
```


### Exploring and presenting the dataset

The merged data set contains sampled **respondents and non-respondents**. It contains a total of `r dim(data)[[1]]` units and `r dim(data)[[2]]` variables.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
dim(data)
```

The data set contains information about `r nrow(data %>% dplyr::filter(interva == "Complete and valid interview related to CF"))` respondents and `r nrow(data %>% dplyr::filter(interva != "Complete and valid interview related to CF"))` non-respondents.


And this is a list of the variables it contains (with their labels). **idno** is the individual identification variable.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
cbind(names(data),attributes(data)$variable.labels) %>% 
  as_data_frame() %>%
  print()
```

Here I will try to give population estimates for cigarette and alcohol consumption in the UK. These will be our 'y' variables or variables of interest. The information for these variables was obtained from survey responses. The idea is to first give descriptives of the distribution of these two variables such as quantiles and mean. Then I will do a simple extrapolation and compute total cigarette and alcohol consumption for the whole UK.

These are our y variables:

* __cgtsday__: Number of cigarettes smoked on a typical day.
* __alcohol_day__: Grams of alcohol ingested on a daily basis. Computed in the *Recoding* section from the amount of alcohol drank last time during weekdays and weekend.


```{r, echo=TRUE, warning=FALSE, message=FALSE}
data[resp.y] %>%
  head(10)
```


#### Paradata variables

Our survey contains variables which give information about the data collection process. First, we have some variables that come from the 'sample data (SDDF)' file. These contain info about the 'primary sampling unit' and the probability of each unit of being selected in the sample. These two variables are only available for respondents. In most projects we would have to compute the probability of being sampled by ourselves.   

* __psu__: This variable includes information on the primary sampling unit (PSU). In the UK this refered to the 'postcode address file'.
* __prob__: Probability of being included in the sample (i.e. approached for survey).

```{r, echo=TRUE, warning=FALSE, message=FALSE}
data[vars.sample.data] %>%
  head(10)
```

The dataset also contains variables for all sampled units (i.e. respondents and non-respondents). These give information  about the events that occurred during the data collection process. We will use these variables as covariates during the computation of **Non-response weights** in step two.  

* __typesamp__: Refers to the type of unit sampled. In the UK, all units sampled were addresses. In other countries, households and individual people were sampled.   
* __interva__: Shows the final outcome of the contact. In the UK sample, only codes 'Complete ...' and 'No interview ...' were used.
* __telnum__: The interviewed person gave his/her mobile phone to the interviewer.
* __agea_1__:  Interviewer estimation of age of respondent or household member who refuses to give the interview.
* __gendera1__: Interviewer estimation of gender of respondent or household member who refuses to give the interview.
* __type__: Type of house sampled unit lives in. 
* __access__: Entry phone or locked gate/door before reaching respondent's individual door.
* __physa__: Interviewer assessment overall physical condition building/house.
* __littera__: Interviewer assessment of amount of litter and rubbish in the immediate vicinity.
* __vandaa__: Interviewer assessment of amount of vandalism and graffiti in the immediate vicinity.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
data[vars.paradata] %>%
  head(10)
```


#### Survey responses

Apart from the variables of interest (cigarette and alcohol consumption) our dataset has other variables obtained from survey responses. Obviously, these are only available for respondents. We will try to use these variables to calibrate the survey in __Use of auxiliary data/calibration__ step. Some of these variables are:

* __vote__: Voted last national election (Yes/No)
* __prtvtbgb__: Party voted for in last national election
* __prtclbgb__: Which party feel closer to, United Kingdom
* __prtdgcl__: How close does the repondent feel to the party party from 'prtclbgb'
* __ctzcntr__: Has UK citizenship (Yes/No)
* __ctzshipc__: Citizenship of respondent
* __brncntr__:Respondent born in the UK
* __cntbrthc__: Respondent country of birth
* __gndr__: Gender of respondent
* __agea__: Calculated age of respondent
* __eisced__: Highest level of education of respondent
* __pdwrk__: In paid work
* __edctn__: In education
* __uempla__: In unemployment, actively looking for a job
* __uempli__: In unemployment, not actively looking for a job
* __rtrd__: Retired
* __wrkctra__: Employment contract unlimited or limited duration
* __hinctnta__: Household's total net income, all sources


```{r, echo=TRUE, warning=FALSE, message=FALSE}
data[c(resp.id, resp.x)] %>%
  head(10)
```


This is how the whole dataset looks like:


```{r, echo=TRUE, warning=FALSE, message=FALSE}
head(data, 10)
```


## Step 1: Design weights

The first step in weighing is to take into account the differences in the probability of being selected for the sample. The European Social Survey (ESS) did not use a register of people in the UK (in other countries they did). They first selected postcode sectors from the Post Office’s small user
postcode address file (PAF), merging smaller sectors. The probability of each PAF of being selected was proportional to the number of addresses it contained. Then, they selected 20 addresses inside of each sampled PAF and a dwelling for each address. For each dewlling they selected a household and then a person in each household. The full explanation of the sampling procedure is given in page 163 of The data documentation report ([Edition 3.1](http://www.europeansocialsurvey.org/docs/round7/survey/ESS7_data_documentation_report_e03_1.pdf) ). 

This sampling design is most probably done because they had a list of addresses but not households or individuals. If we don't weight this survey, we would probably over-represent people in addresses that have smaller number of dewellings, dwellings that include smaller number of households and households that comprise smaller number of people.

Fortunately for us, the probablity of each respondent of being sampled was included in the ESS dataset. In many projects, however, we will have to compute sampling probailities ourselves. A basic but **important test** that should be performed after computing the probabilities is making sure that all probabilities are between 0 and 1.

```{r, echo=TRUE, warning=FALSE, message=FALSE}

probabilities <- data %>%
  summarise(min.probability = min(prob, na.rm = T),
            max.probability = max(prob, na.rm = T)) %>%
  as.vector()

print(probabilities)

if(probabilities$min.probability < 0){stop("Minimum probability of being sampled is smaller than 0. Review sampling probabilities before computing base weights.")}else if(probabilities$max.probability > 1){stop("Maximum probability of being sampled is larger than 1. Review sampling probabilities before computing base weights.")}

rm(probabilities)

```


We see that there are actually `r length(unique(data$prob))` unique sampling probabilities computed in the dataset. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}
unique(sort(data$prob))

```

The vast majority of respondents had probability of `r sort(table(data$prob), decreasing = T)[[1]]` or `r sort(table(data$prob), decreasing = T)[[2]]`. This would mean that the most frequent probabilites of inclusion were of `r 1/round(sort(table(data$prob), decreasing = T)[[1]],0)` and `r 1/round(sort(table(data$prob), decreasing = T)[[2]],0)` respectively.
We see a minority of around 15% of observations with smaller probabilities. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}
ggplot(data, aes(x = prob)) +
  geom_histogram()

```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
table(round(data$prob*100,6))
```


We see that 


```{r, echo=TRUE, warning=FALSE, message=FALSE}

data %>%
  group_by(type) %>%
  summarise(n = n(),
    mean.prob.percentage = mean(prob, na.rm = T)*100) %>%
      arrange(desc(mean.prob.percentage))

```


```{r, echo=TRUE, warning=FALSE, message=FALSE}
data %<>%
  mutate(hhmmb.factor = as.factor(hhmmb) %>% fct_recode(`+5` = "6",
                                                        `+5` = "7",
                                                        `+5` = "8"))

data %>%
  filter(!is.na(hhmmb.factor)) %>%
  group_by(hhmmb.factor) %>%
  summarise(n = n(),
    mean.prob.percentage = mean(prob, na.rm = T)*100) %>%
      arrange(desc(mean.prob.percentage))

```

The base weights are equal to the inverse of the probability of inclusion. Therefore, the base weight (*d_{0}*) of a respondent (*i*) will be equal to: $d_{0i} =  1/\pi_{i}$. 


Here we compute the base weight from the probability given in the ESS database.   

```{r, echo=TRUE, warning=FALSE, message=FALSE}

data %<>%
  mutate(base.weight = 1/prob)

data %>%
  select(prob, base.weight) %>% head(10)

```

A simple interpretation of base weights it 'the number of units in our population that each unit in our sample represents'. There is a simple but **important test** that we should perform after computing base weights. The sum of all base weights should be equal to the total number of units in our population. The ESS dataset for UK only included sampling probabilities for respondents (i.e. sampled units that responded to the survey!) but they did not include sampling probabilities of non-respondents.  I would guess that this is because sampling probability depends on information that is obtained from the interview (i.e. number of people in household, number of households in dwelling, etc.).  Not knowing the sampling probability for some sampled units is not an optimal situation. 

The sum of our computed weights in the ESS dataset with `r table(data$interva)[["Complete and valid interview related to CF"]]` respondents equals `r data %>%  summarise(sum.base.weights.ess.dataset = round(sum(base.weight, na.rm = T),0)) %>% .[["sum.base.weights.ess.dataset"]]`. Doing a very simple Extrapolation to include the `r table(data$interva)[["No interview for other reason"]]` non-respondents would give us a sum of weights equal to `r data %>%  summarise(sum.base.weights.ess.dataset = round(sum(base.weight, na.rm = T),0)) %>% .[["sum.base.weights.ess.dataset"]]* (nrow(data)/ table(data$interva)[["Complete and valid interview related to CF"]])`. This last figure would be much closer to the total UK population over 15. 

It is a common practice for many researchers to scale the weights so that their sum equals the sample size (instead of the population size). Scaled weights would equally adjust for differences in sampling probabilities. 

Here we compute our scaled weights and we compare them with the ones given in the ESS dataset. Here we see that our weights scaled (*base.weigth.scaled*) are almost equal to those computed in the ESS dataset (*dweigth*). The small differences are probably due to rounding error.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
data %>%
  filter(!is.na(base.weight)) %>% 
  select(idno, base.weight) %>%
  mutate(base.weight.scaled = base.weight/sum(base.weight, na.rm = T)*nrow(data[!is.na(data$prob),])) %>%
  left_join(original.weights %>% select(idno, dweight),
            by = "idno") %>% head(10)

data %<>%
  mutate(base.weight.scaled = base.weight/sum(base.weight, na.rm = T)*nrow(data[!is.na(data$prob),]))


```

As I mentioned before, base weights should sum up to the entire population from which the sample is drawn or to the total number of respondents if scaled (as they did in the ESS).  


## Step 2: Non-response weights

### What are of non-response weights?

The second basic step in weighting a survey is accounting for differences in the propensity to respond. Imagine a situation in which a profile of sampled units had higher propensity to respond than another profile. Imagine as well that the characteristics of both profiles were associated to our y variables (here alcohol and cigarretes consumption). This would create a bias in our analyses. 

If we compute the sample ourselves, we can also compute the probability of a unit of being sampled. Computing the probability of replying to the survey is, however, more challenging. As we can not direclty observe the probability of a unit of replying to the survey, we need to estimate it. This is done using information which we know for both respondent and non-respondent units. Here it is useful to think about the probability of response as a latent (i.e. not directly observable) variable.

There are two main ways of using this information. The first one would be creating cells from variable intersections (i.e. sampled units 15-24 & countryside; 15-24 & cities; 25-34 & countryside, etc.) and then calculate the probability of response in each cell. The second method is to estimate the probability of response by modelling it. 

The first approach has the advantage of being more simple. However, computing a large number of cells from crossing variable could most probably lead to having empty cells or cells with a very small number of sampled units. The probability estimated for these cells with smaller number of units could be far from the real one. We could apply it if we thought that the probability of responding can actually be explained by the variable intersections we used.

The second approach relies on the modelling skills of the researcher computing the weights. In order to estimate the proability of response we need to predict it. 'Predictive modelling' is slightly different than the usual 'inference modelling'. Here we are not interested in understanding the causes of differences in response propensities. We are just interested in predicting these. Therefore, we should take a 'data driven' approach that differs from those we usually see in social sciences. For an excellent introduction to 'predictive statistics' you can check the free book ['An Introduction to Statistical Learning' by James et al.](http://www-bcf.usc.edu/~gareth/ISL/).

This is not a guide on 'predictive modelling'. However, it might be worth it to very briefly explain the basic principle behind it. We should try to build a model which is able to estimate the probability using only 'relevant' information and excluding 'noise' from the model. Therefore, a predictive model should be fitting the observed data well enough but at the same time not too specific to it. 

For this specific case of non-response weighting, we are especially interested in using propensity predictors which are related to both the response propensity and our dependent variables.

Here we will use the paradata information to model the probability of response. The variables describing the type of house and the immediate vicinity around houses have a relatively small number of missing values (~ 8%) these missings seem to be related (i.e. all columns missing). Those sampled units that have missing values on all these paradata variables are always non-respondents. It would be useful to know why this pattern exists. I would guess that these are sampled units which interviewers could not reach for some reason. For this analysis, we will not use these units with missing values in ALL paradata variables. This should only result in a linear transformation of the estimated probabilities of response. Moreover, we can later adjust our model to 'undo' this transformation.


```{r, echo=TRUE, warning=FALSE, message=FALSE}

data.nonresponse.model <- data[vars.paradata] %>% select(idno, interva, type:vandaa)

data.nonresponse.model$all.missing <- NA

data.nonresponse.model$all.missing[is.na(data.nonresponse.model$type) &
                                     is.na(data.nonresponse.model$access) &
                                     is.na(data.nonresponse.model$physa) & 
                                     is.na(data.nonresponse.model$littera) &
                                     is.na(data.nonresponse.model$vandaa)] <- 1

data.nonresponse.model %<>%
  filter(is.na(all.missing)) %>%
  select(-all.missing)

indep.vars.nonresponse <- c("type", "access", "physa", "littera", "vandaa")

data.nonresponse.model[,c("type", "access")]  %<>% map(function(x){as.character(x)})

#data.nonresponse.model %>% map(function(x){sum(is.na(x))})

# Missing category for missings in multinomial variables.
  
for(i in c("type", "access")) {
  data.nonresponse.model[[i]][is.na(data.nonresponse.model[[i]])] <- "Missing"
  
}

# Mean imputation for ordinal variables.

for(i in c("physa", "littera", "vandaa")) {
  data.nonresponse.model[[i]][is.na(data.nonresponse.model[[i]])] <- levels(data.nonresponse.model[[i]])[median(data.nonresponse.model[[i]] %>% as.numeric(), na.rm = T) ] 
  
}

for(i in c("physa", "littera", "vandaa")) {
  data.nonresponse.model[[i]] <- as.ordered(data.nonresponse.model[[i]])

  
}

data.nonresponse.model %<>%
  mutate(response = as.numeric(as.numeric(interva) == 1))

data %<>%
  mutate(response = as.numeric(as.numeric(interva) == 1))


```

### Estimating response propensities using logistic regression

Valliant et al (2013) recommend estimating the response propensities and then grouping them in classes. This should avoid extreme weights. One way of estimating the response propensities is using logistic regression. This logistic regression should be unweighted. Later in this section we will try other ways of computing estimates of response propensities. 

In order to do a Logistic regression in R, we need to specify the dependent variable (response) and predictors (type, access, physa, littera and vandaa) in a formula. Then we input the formula and the dataset into the 'lm' function. 

```{r,  warning=FALSE, message=FALSE}
formula <- as.formula("response ~ type + access + physa + littera + vandaa")

options(na.action = 'na.pass')

x.matrix <- model.matrix(formula, data = data.nonresponse.model)[, -1]

log.reg.m <- glm(formula,  
  data = data.nonresponse.model,
  family = "binomial")

coef.response.log <- coef(log.reg.m)

predicted.log <- log.reg.m$fitted.values

data.nonresponse.model$predicted.log <- predicted.log

predicted.log %>% head()

```

As explained before, we are now computing our estimates from a subset of sampled units. This does not take into account many non-respondents which had missing values in all predictors of our non-response model. This means that the previous propensities are slightly overestimated. This is because the response rate in our full dataset is `r mean(data$response) %>% round(3)` and in our subset used for non-response adjustment is `r mean(data.nonresponse.model$response) %>% round(3)`. 
This is not entirely related to non-response weight adjustment so I will just compute the new estimates with modified intercept in the next chunk and I will not explain in depth how to perform the trasformation. For an excellent description on how it's done I will refer to Stanford University's online course on [SP Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/info).

Below the next chunk of code you can see the first six transformed estimated response propensities.

```{r,  warning=FALSE, message=FALSE}

resp.pop <-  mean(data$response)
  
resp.subgroup <- mean(data.nonresponse.model$response)

log.reg.m$coefficients[1] <- coef(log.reg.m)[[1]] + log(resp.pop/(1-resp.pop) - log(resp.subgroup/(1-resp.subgroup)) )

rm(resp.pop, resp.subgroup)

predicted.log.tr <- predict(log.reg.m, data.nonresponse.model, type = "response")

data.nonresponse.model$predicted.log.tr <- predicted.log.tr

predicted.log.tr %>% head()

rm(coef.response.log, formula, i)

```

### Creating non-response adjustment classes


Here I'll follow the procedure for creating classes explained in pag. 329 of Valliant et al. It is not so clear how many classes should be created.

Looking at the distribution of estimated probabilities of response, we observe a large majority of values between `r quantile(predicted.log, probs = 0.2) %>% round(1)` and `r quantile(predicted.log, probs = 0.9) %>% round(1)`. However, there are several outliers at both ends of the distribution. 

As there is not so much dispersion in values in the middle of the distribution, creating classess accoring to quintiles might not be the best way to account for differences in estimated response propensities. However, other methods might create classess which are too specific to outliers. This is kind of a bias-variance trade off. If we fit broad classes which encompass very different estimated probabilities within them, we will be adjusting less and so keeping more bias in our estiamtes. If we create tight classes capturing these outliers, then we will have  large differences in weights and so more variance in our estimates. 


```{r,  warning=FALSE, message=FALSE}

predicted.log.tr %>% quantile(probs = seq(0,1,0.05)) %>% round(4)

```


These are the 5 classes created using 20th, 40th, 60th and 80th quintiles.


```{r,  warning=FALSE, message=FALSE}

data.nonresponse.model$predicted.class <- cut(x = data.nonresponse.model$predicted.log.tr, breaks = c(0,quantile(predicted.log.tr, probs = seq(0,1,0.2))[-1] ))

data.nonresponse.model$predicted.class %>% levels()

```

And below there is a summary of the estimated propensities included in each of them (in boxplots).We can see that the first and last groups have more dispersion in propensities. The middle three groups have very little dispersion and are similar between them.

```{r,  warning=FALSE, message=FALSE}

ggplot(data.nonresponse.model, aes(x = predicted.class, y = predicted.log.tr)) +
  geom_boxplot()

```


To compute the non-response weights, we can use the mean estimated probability of response in each class. 

```{r,  warning=FALSE, message=FALSE}

data.nonresponse.model %>%
  group_by(predicted.class) %>%
  summarise(mean.prob = mean(predicted.log.tr) %>% round(4))

```

And then we can compute the non-response weight as the inverse of the mean probabilities in each class. 

```{r,  warning=FALSE, message=FALSE}
data.nonresponse.model %<>% left_join(
data.nonresponse.model %>%
  group_by(predicted.class) %>%
  summarise(mean.prob = mean(predicted.log.tr) ), by = "predicted.class")

data.nonresponse.model %<>%
  mutate(nonresp.weight = round(1/mean.prob, 4) )

data.nonresponse.model$nonresp.weight %>% unique %>% sort

```

### Testing non-response adjustment classes

After creating the classes, a good practice would be to check for covariate balance. This procedure is explained in pag. 330 of Valliant et al. Units in the same class should have similar values in covariates. At the same time, we would ideally find differencess between classes. In plain words, here we would check if classe are made of homogeneous units (within-class check) and if classes really distinguish different profiles of these (between-class check). For the within-group check, we are especially interested in checking if profiles of respondents and non-respondents within each class are similar. 

In my opinion, the best way of doing this analysis is by fitting two different regressions for each covariate. As an example I will do the balance analysis for the ordinal covariate *physa*. In a real analysis this should be repeated for all covariates.

For the between-class check, we can fit a model only with class predictors. This should show if classes explain differences in covariate variables. As we have an ordinal covariate (*physa*), I will use al ordered logistic regression. An alternative would be to treat the covariate as a continous varaible and use an OLS regression. 

The variable *physa* compares the 'Overall physical condition building/house'. The variable is negativelly coded so larger values mean worse physical condition. The coefficients of the ordered logistic regression show that the larger the estimated propensity, the smaller the probability of being in bad physical conditions. Therefore, people in houses with worse physical conditions would have smaller response propensity and so be underrepresented in our sample. From the coefficients below we see that the classes we have created for non-response adjustment somehow explain differences in our analysed covariate. However, ideally we would have a more clear effect of classes on our covariate. We see that the second adjustment class created (that for estimated propensities between 0.41 and 0.443) actually has larger probabilities of having worse physical conditions than the base category (0 to 0.41). At the same time, the third and fourth categories (0.443 to 0.459) and (0.459 to 0.485) have very similar coefficients, which might indicate that they do not really distinguish different classes of sampled units. 


```{r,  warning=FALSE, message=FALSE}

formula. <- as.formula("physa ~ predicted.class")

test.between.model <- polr(formula = formula., data = data.nonresponse.model, method = "logistic")

ctable.between <- coef(summary(test.between.model))

p <- pnorm(abs(ctable.between[, "t value"]), lower.tail = FALSE) %>% round(4)

ctable.between <- cbind(ctable.between, "p value" = p)

ctable.between

```

For the within-class check, we can extend our model to include interactions between class and response. This will check if, within a non-response class, there are differences between respondents and non-respondents in our covariate *physa* (ideally they wouldn't be and we would not find them).

In this second test (see coefficients below) we see that one of the interactions has a significant coefficient at 5% confidence level. Ideally, all interaction terms would be non-significant, meaning that we do not observe within-group differences between respondents and non-respondents in our covariates. A way of dealing with this situation with one significant coefficient would be to explore other ways of spliting units into classes. However, as explained before, these other categorisations would have drawbacks in terms of inflated variance. Moreover, if we have a large number of classes and covariates, we would expect to find significant coefficients just by chance. Therefore, as long as these unbalances are not extended most classes and covariates, reporting these unbalances should be enough.


```{r,  warning=FALSE, message=FALSE}

formula. <- as.formula("physa ~ response + predicted.class + response * predicted.class")

test.within.model <- polr(formula = formula., data = data.nonresponse.model, method = "logistic")

ctable.within <- coef(summary(test.within.model))

p <- pnorm(abs(ctable.within[, "t value"]), lower.tail = FALSE) %>% round(4)

ctable.within <- cbind(ctable.within, "p value" = p)

ctable.within

rm(formula., test.within.model, p, indep.vars.nonresponse, i, test.between.model,
   ctable.between, ctable.within)

```

### Propensity adjustment (alternative to adjustment classes)

An alternative to computing class adjustments is to directly use the inverse of the estimated probabilities of response. Adjustment classes act are a way of 'smoothing' predictions, avoiding extreme values and overfit. Therefore, they are based in some kind of mistrust of statistical models. Even if they are a standard procedure in survey methodology and backed by solid literature, they might look a bit naive to researchers coming from other areas (e.g. econometrics, genomics, etc.). Adjusting directly with propensity scores relies more on statistical/modelling skills of researchers. Here we will show a couple of alternative methods. As previously explained, these need to avoid being too specific to our data. Instead, they should try to give information on the distribution that generated this data.

#### Estimating response propensities using Cross-validated LASSO Regression.

The first method we will try is a penalised regression, more specifically a LASSO regression. Penalised regressions are very similar to a logistic regression but have the particularity of having 'shrinked' coefficients. This reduces the influence of predictors to avoid overfit. In other words, it tries to avoid noise from our data set into the model. This is done by tunning the penalty parameters with cross-validation (a resampling method).

The first six estimated propensities are after the chunk of code. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}

formula. <- as.formula("response ~ type + access + physa + littera + vandaa")

options(na.action = 'na.pass')

x.matrix <- model.matrix(formula., data = data.nonresponse.model)[, -1]

glm.cv.model <- cv.glmnet(x.matrix, data.nonresponse.model$response, alpha = 1, family="binomial")

predicted.lasso <- predict(glm.cv.model, x.matrix, s = "lambda.min", type = "response")[,1]

data.nonresponse.model$predicted.lasso <- predicted.lasso

head(predicted.lasso)

```

Comparing the predictions obtained with these two methods (logistic regression and cross-validated lasso ) we observe that they all give relatively similar values. They are all centered around the same mean, which is the proportion of respondents in our subset. Lasso regression is the more 'rigid'/'smoother' with a smaller standard deviation in its predictions. Logistic regression is almost always slightly more distant of the mean than the lasso regression. It is important to note that our logistic and lasso regressions included only linear parameters (i.e. no squared coefficients nor interactions). Therefore, these were rather 'rigid' methods.


```{r,  echo=TRUE, warning=FALSE, message=FALSE}

mean(data.nonresponse.model$response)

list.comparison <- list()

list.comparison$head.predicted.vals <- cbind(predicted.log, predicted.lasso) %>% head(10)

list.comparison$means <- c(mean.log.reg = mean(predicted.log), 
                               mean.lasso = mean(predicted.lasso)) 

list.comparison$sd <- c(sd.log.reg = sd(predicted.log), 
                               sd.lasso = sd(predicted.lasso)) 

list.comparison

rm(list.comparison)
```

Comparing the fit of both models (below the next chunk), we see that the logistic regression fits a bit better to the whole sample, with 57.6% of success in classifying sample units as respondents vs non-respondents. However, the difference is almost negligible. Moreover, a 58% success in classifying is a rather poor model (by chance we would already expect to correctly classify 50%).

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

data.nonresponse.model %<>%
  mutate(predicted.category.log.reg = (predicted.log > 0.5) %>% as.numeric,
         predicted.category.lasso = (predicted.lasso > 0.5) %>% as.numeric)

train.correct.logreg <- table(data.nonresponse.model$response, data.nonresponse.model$predicted.category.log.reg) %>% diag() %>% sum()/nrow(data.nonresponse.model)

train.correct.lasso <- table(data.nonresponse.model$response, data.nonresponse.model$predicted.category.lasso) %>% diag() %>% sum()/nrow(data.nonresponse.model)

c(train.correct.logreg = train.correct.logreg, train.correct.lasso = train.correct.lasso)

```

The two chunks below compute the cross-validated ratio of correctly classified units. We would expect these to be lower than the successfuly classified units for the whole sample. This is because cross-validation is a resampling method. We use it to get an idea of how our estimates (e.g. ratio of correctly classified units) would fit the whole statistical population (and not only the sample at hand). 

Cross-validated ratio of correctly classified units would vary each time we run the procedure. This will not happen here because I set a seed for random procedures at the beginning or the script. We observe that, for both methods, the cross-validated ratio of correctly classified units is slightly lower than the estimate for our sample. Also, both methods have a very similar predictive capacity. 

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

folds <- createFolds(data.nonresponse.model$response, k = 10, list = TRUE, returnTrain = FALSE)

formula. <- as.formula("response ~ type + access + physa + littera + vandaa")

  success.rate.logreg <- vector()

for(i in 1:length(folds)){
  
  t <- i
  
  train.folds <- c(1:10)[-t]
  
  temp.train.data <- data.nonresponse.model[folds[train.folds] %>% unlist(),]
  
  temp.test.data <- data.nonresponse.model[folds[t] %>% unlist(),]
  
  temp.log.reg.m <- glm(formula., data = temp.train.data, family = "binomial")
  
  temp.predicted <- predict(temp.log.reg.m, temp.test.data, type = "response")
  
  temp.predicted <- ifelse(temp.predicted > 0.5, 1, 0)
  
  success.rate.logreg <- c(success.rate.logreg, table(temp.predicted, temp.test.data$response) %>% diag() %>% sum()/nrow(temp.test.data) )
  
  
}

  cv.success.log.reg <- success.rate.logreg %>% sum()/length(success.rate.logreg)

  rm(train.folds, temp.train.data, temp.test.data, temp.log.reg.m, temp.predicted)

    
```


```{r,  echo=TRUE, warning=FALSE, message=FALSE}

  success.rate.glmnet <- vector()

for(i in 1:length(folds)){
  
  t <- i
  
  train.folds <- c(1:10)[-t]
  
  temp.train.data <- x.matrix[folds[train.folds] %>% unlist(),]
  
  temp.train.y  <- data.nonresponse.model[folds[train.folds] %>% unlist(),"response"]

  temp.test.data <- x.matrix[folds[t] %>% unlist(),]
  
  temp.test.y <- data.nonresponse.model[folds[t] %>% unlist(),"response"]

  temp.glmnet.m <- glmnet(x = temp.train.data, y = temp.train.y, family = "binomial")
  
  temp.predicted <- predict(temp.glmnet.m, temp.test.data, type = "response", s = glm.cv.model$lambda.min)
  
  temp.predicted <- ifelse(temp.predicted > 0.5, 1, 0)
  
  success.rate.glmnet <- c(success.rate.glmnet, table(temp.predicted, temp.test.y) %>% diag() %>% sum()/nrow(temp.test.data) )
  
}

  cv.success.glmnet <- success.rate.glmnet %>% sum()/length(success.rate.glmnet)

  rm(t, train.folds, temp.train.data, temp.train.y, temp.test.data,
     tempt.test.y, temp.glmnet.m, temp.predicted)  
  
  c(cv.success.log.reg = cv.success.log.reg, cv.success.glmnet = cv.success.glmnet)
  
```

If we wanted to do an adjustment directly using the estimated propensity scores and without computing adjustment classes, we would just use the inverse of the estimated propensities as the non-response weights. It would be a good idea to use the Lasso predictions as these are less sparce than those from the logistic regression. Therefore, we might have less variance inflation with (almost) the same expected reduction in bias.

If we had no information about population estimates, we would end the weighting procedure here. The 'final weight' would be the multiplicaiton of both base scaled weight and the scaled non-response weight. I will call this new weights 'final weights' although we still have to perform adjustments to them and so will not really be 'final'. 

First I will include the computed non-response weights using adjustment classes to the main 'data' dataset. Then I will drop all non-respondents. This is because we are not going to use them any more in the next steps of our analysis. After that, I will scale the non-response weights to the sample of respondents and multiply the (scaled) design weights and the (scaled) non-response weights. 

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

data %<>%
  left_join(data.nonresponse.model %>% select(idno, nonresp.weight),
            by = "idno")

data %<>%
  filter(response == 1, !is.na(base.weight.scaled)) %>%
  mutate(nonresp.weight.scaled = nonresp.weight/mean(nonresp.weight),
         final.weight = base.weight.scaled * nonresp.weight.scaled)

rm(x.matrix, cv.success.glmnet, cv.success.log.reg, folds, formula.,
   glm.cv.model, i, log.reg.m, predicted.lasso,predicted.log.tr, success.rate.glmnet, success.rate.logreg, train.correct.lasso, train.correct.logreg)

```


## Step 3: Use of auxiliary data for weight calibration 

In certain cases we might have information about our statistical population (e.g. census counts or proportions from official statistics). We can then use these to 'correct' our weights. This adjustment is called calibration and consists on finding a new set of weights that are as near as possible the input ('final') weights but reproduce the population information exactly. Valliant et al 2013 explain that using the previous 'final' weights as input for the calibration step allows us to 'borrow' good estiamtion properties from those. 

An important difference between this step of using auxiliary data for calibration and the previous one on non-response adjustment is that non-response adjustment requieres having information for both sampled respondents and non-respondents. Calibration only requieres information for respondents and population in general. 

Here I will calibrate weights using a 'raking' procedure (explained in Valliant el al 2013 page 358). A practical difference between this method and other ones is that it does not requiere information on cross-classification categories but just marginal population counts. In other words, we do not need the information from crossing several variables (although we can use it if available). As explained by Lumley (2010, page 139), the process is very much iterative. It involves post-stratifying on each set of variables in turn, and repeating the process until the weights stop changing. 

The ESS used cross-classifications of age group and gender plus region (separately) to calibrate UK data. Here we will try to reproduce their calibration. For more information on ESS post-stratification weights see their document [Documentation of ESS
Post-Stratification Weights](http://www.europeansocialsurvey.org/docs/methodology/ESS_post_stratification_weights_documentation.pdf)

First we will compute the interaction between gender and age with the categories used for ESS calibration. 

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

data %<>%
  mutate(agea_rec = cut(agea, breaks = c(14, 34, 54, 99))) %>%
         unite(col = gender_age_rec, gndr, agea_rec, remove = F) 

```

These are the total number of weighted units in each of the categories we will use for calibration can be seen in the tables below.

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

data$agea %>% is.na() %>% sum() # Need to address these 20 nas. DELETE!

data %>%
  group_by(gender_age_rec) %>%
  summarise(total_n_sample = sum(final.weight))

data %>%
  group_by(region) %>%
  summarise(total_n_sample = sum(final.weight))

```

Now we import Eurostat data and recode it into ESS adjustment/post-stratification categories.

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

age.gender.eurostat <- read_delim("data/Eurostat/Agebygender_demo_pjangroup.csv", delim = ",")

age.gender.eurostat %>%
  write_csv("data/Eurostat/Agebygender_demo_pjangroup.csv")

age.gender.eurostat %<>%
  spread(key = Age, value = Value)

age.gender.eurostat %<>%
  mutate(`15to34` = `From 15 to 19 years` + `From 20 to 24 years` +  `From 25 to 29 years` +
           `From 30 to 34 years`,
         `35to54` = `From 35 to 39 years` + `From 40 to 44 years` + `From 45 to 49 years` + 
           `From 50 to 54 years`,
         `55to99` =  `From 55 to 59 years` + `From 60 to 64 years` + `From 65 to 69 years` +
           `From 70 to 74 years` + `75 years or over`) %>%
  select(SEX, `15to34`:`55to99`)

```


```{r,  echo=TRUE, warning=FALSE, message=FALSE}

region.eurostat <- read_delim("data/Eurostat/Nuts2byage.csv", delim = ",")

region.eurostat %<>%
  gather(key = age, value = population, -Country) 

region.eurostat %<>%
  group_by(Country) %>%
  summarise(pop_sum = sum(population) )
  
```

I will now scale the Eurostat data to our sample size. The idea is to obtain the weights that make our sample proportions look like those in Eurostat. For these, we will calculate how many respondents in our sample should pertain to each category if we had Eurostat proportions. We will use our sample size based only on (weighted) completed observations in our post-stratification adjustment variables (age, gender and region).

Total (weighted) observations in our sample with completed observations:

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

data.calibration <- data %>%  
  filter(!is.na(agea_rec), !is.na(gndr), !is.na(region))

weighted.pop <- sum(data.calibration$final.weight)

weighted.pop

```

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

age.gender.eurostat %<>%
  gather(key = age, value = population, -SEX) %>%
  unite(col = gender_age_rec, SEX, age) 

total.population <- age.gender.eurostat$population %>% 
  sum()

age.gender.eurostat %<>%
  mutate(Freq = round(population/total.population * weighted.pop, 0) ) %>%
  select(-population)

age.gender.eurostat$gender_age_rec <- c("Female_(14,34]", "Male_(14,34]",
                                        "Female_(34,54]", "Male_(34,54]",
                                        "Female_(54,99]", "Male_(54,99]")

rm(total.population)

```

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

total.population <- region.eurostat$pop_sum %>%
  sum()

region.eurostat %<>%
  mutate(Freq = round(pop_sum/total.population * weighted.pop, 0) ) %>%
  select(-pop_sum)

names(region.eurostat)[[1]] <- "region" 

region.eurostat$region[region.eurostat$region == "East Midlands (UK)"] <- "East Midlands (England)"
region.eurostat$region[region.eurostat$region == "North East (UK)"] <- "North East (England)"
region.eurostat$region[region.eurostat$region == "North West (UK)"] <- "North West (England)"
region.eurostat$region[region.eurostat$region == "Northern Ireland (UK)"] <- "Northern Ireland"
region.eurostat$region[region.eurostat$region == "South East (UK)"] <- "South East (England)"
region.eurostat$region[region.eurostat$region == "South West (UK)"] <- "South West (England)"
region.eurostat$region[region.eurostat$region == "West Midlands (UK)"] <- "West Midlands (England)"
region.eurostat$region[region.eurostat$region == "Yorkshire and The Humber"] <- "Yorkshire and the Humber"


```

```{r,  echo=TRUE, warning=FALSE, message=FALSE}

if( identical(region.eurostat$region %>% unique %>% sort, data.calibration$region %>% as.character() %>% unique %>% sort) != T) {
    stop("Levels in region variable have to be the the same in the calibration and dataset used for population frequencies")
}

if( identical(age.gender.eurostat$gender_age_rec %>% unique %>% sort, data.calibration$gender_age_rec %>% as.character() %>% unique %>% sort) != T) {
    stop("Levels in age by gender categories variable have to be the the same in the calibration and dataset used for population frequencies")
}

```


Now it's time to calibrate! 

Explain a bit raking and stuff!

I do it twice. Once to reproduce ESS weights and the other to compute my own weights which also include nr modelling. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}

our.svydesign <- svydesign(ids = ~ 0, weights = ~final.weight, data = data.calibration)

ess.svydesign <- svydesign(ids = ~ 0, weights = ~base.weight, data = data.calibration)

our.raked <- rake(our.svydesign, sample.margins = list(~region, ~gender_age_rec), 
     population = list(region.eurostat, age.gender.eurostat))

ess.raked <- rake(ess.svydesign, sample.margins = list(~region, ~gender_age_rec), 
     population = list(region.eurostat, age.gender.eurostat))

```


```{r, echo=TRUE, warning=FALSE, message=FALSE}

raked.weight <- our.raked$postStrata[[1]][[1]] %>% attributes() %>% .[["weights"]]

ess.raked.weight <- ess.raked$postStrata[[1]][[1]] %>% attributes() %>% .[["weights"]]

data.calibration$ess.raked.weight <- ess.raked.weight

data.calibration$raked.weight <- raked.weight

```

Test calibrated weights 

Age by gender

```{r, echo=TRUE, warning=FALSE, message=FALSE}

left_join(
data.calibration %>%
  group_by(gender_age_rec) %>%
  summarise(calibrated.sample.pop = sum(raked.weight)),
age.gender.eurostat)

```

```{r, echo=TRUE, warning=FALSE, message=FALSE}

left_join(
data.calibration %>%
  group_by(region) %>%
  summarise(calibrated.sample.pop = sum(raked.weight)),
region.eurostat)

```

EXPLAIN: Merge calibrated weights back to the original dataset. Use final weights (sampling * non-response weights) for those units with missings in calibration vars. 

```{r, echo=TRUE, warning=FALSE, message=FALSE}

data %<>%
  left_join(data.calibration %>% select(idno, raked.weight, ess.raked.weight), by = "idno")

data$raked.weight[is.na(data$raked.weight)] <- data$final.weight[is.na(data$raked.weight)]

data$ess.raked.weight[is.na(data$ess.raked.weight)] <- data$final.weight[is.na(data$ess.raked.weight)]

```

Compare ESS weight, my reproduction of ESS weight and my raked weights

```{r, echo=TRUE, warning=FALSE, message=FALSE}

left_join(
data %>% select(idno, raked.weight, ess.raked.weight),
original.weights %>% select(idno, pspwght)
) %>% head(15)

```

Obviously, my raked weights tend to be a bit further from ESS than those done only with base/sampling weights

```{r, echo=TRUE, warning=FALSE, message=FALSE}
left_join(
data %>% select(idno, raked.weight, ess.raked.weight),
original.weights %>% select(idno, pspwght)
) %>%
  mutate(diff.myraked = pspwght-raked.weight,
         diff.essraked = pspwght-ess.raked.weight) %>%
  summarise(sserrors.myraked = sum(diff.myraked ^ 2),
            sserrors.essraked = sum(diff.essraked ^ 2))

```






ESS: 'The post-stratification weights are obtained by adjusting the design weights in such a way that they will replicate the distribution of the cross-classification of age group, gender, and education in the population and the marginal distribution for region in the population. The population distributions for the adjusting variables were obtained from the European Union Labour Force Survey.'

'For most countries and rounds there is control data for gender, age, education and region.
However, given the available control data it was not in all cases deemed appropriate
to adjust the weighted sample data to the joint distribution of gender, age, education
and region. Hence, it was decided not to use a straightforward post-stratication, but
a raking procedure instead.'

'The raking procedure uses iterative post-stratication to
match weighted marginal distributions of a sample to known population margins. The
software used to calculate the weights is R (R Core Team, 2013) applying the survey
package, (Lumley, 2013).'

## Step 4: Analysis of weight variability

In progress!

## Computing estimates

In progress!

